{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel(r'Model optimization\\2.input\\development dataset.xlsx',sheet_name=\"Sheet 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Halo_Sign'] = data['Halo_Sign'].map({'Exists':1,'Absent':0})  \n",
    "data['Gender'] = data['Gender'].map({'Female':1,'Male':0})\n",
    "data['Composition'] = data['Composition'].map({'Solid':1,'Others':0})\n",
    "data['Shape'] = data['Shape'].map({'Microlobulated':1,'Others':0})\n",
    "data['Echogenicity'] = data['Echogenicity'].map({'Hypoechogenicity':1,'Others':0})\n",
    "data['Echogenic_Foci'] = data['Echogenic_Foci'].map({'Microcalcification':1,'Others':0})\n",
    "data['Margin'] = data['Margin'].map({'Irregular':1,'Smooth':0})\n",
    "data['ATR'] = data['ATR'].map({'Taller_than_Wide':1,'Wider_than_Tall':0})\n",
    "data['Pathological_Diagnosis'] = data['Pathological_Diagnosis'].map({'Malignant':1,'Benign':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded1 = pd.get_dummies(data, columns=['Posterior_Echo'], prefix='')\n",
    "df_encoded1.rename(columns={'_Absent_of_Shadowing':'Absent_of_Shadowing','_Posterior_Attenuation':'Posterior_Attenuation','_Shadowing':'Shadowing'},inplace=True)\n",
    "columns_to_convert1 = ['Absent_of_Shadowing','Posterior_Attenuation','Shadowing']\n",
    "df_encoded1[columns_to_convert1] = df_encoded1[columns_to_convert1].astype(int)\n",
    "\n",
    "\n",
    "df_encoded2 = pd.get_dummies(df_encoded1, columns=['Location'], prefix='')\n",
    "df_encoded2.rename(columns={'_Right_Lobe':'Right_Lobe','_Left_Lobe':'Left_Lobe','_Isthmus':'Isthmus'},inplace=True)\n",
    "columns_to_convert2 = ['Right_Lobe','Left_Lobe','Isthmus']\n",
    "df_encoded2[columns_to_convert2] = df_encoded2[columns_to_convert2].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "order_list = ['Intra_BFS','Peri_BFS']\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for i in order_list:\n",
    "    \n",
    "    df_encoded2[i] = label_encoder.fit_transform(df_encoded2[i])\n",
    "\n",
    "    for class_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):\n",
    "        print(f\"{class_label}: {encoded_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path0 = r'Model optimization\\\\3.output\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_reconde = df_encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "exp_clf = setup(\n",
    "    data_input_reconde, target='Pathological_Diagnosis', session_id=111,\n",
    "    numeric_features=[\"Age\",\"Maximum_Diameter\"],\n",
    "    categorical_features=[\n",
    "    \"Gender\",\"Composition\",\"Shape\",\"Echogenicity\",\"Echogenic_Foci\",\n",
    "    \"Margin\",\"ATR\"], \n",
    "    train_size = 0.7,data_split_shuffle = True,data_split_stratify = True,\n",
    "    ignore_features=[\"ACR\",\"Kwak\",\"Data_Type\",\n",
    "                     \"BMI\",\"Halo_Sign\", \"Absent_of_Shadowing\",\"Posterior_Attenuation\",\"Shadowing\",\n",
    "    \"Right_Lobe\",\"Left_Lobe\",\"Isthmus\",'Intra_BFS'\n",
    "                     ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remove_metric('acc')\n",
    "remove_metric('auc')\n",
    "remove_metric('recall')\n",
    "remove_metric('precision')\n",
    "remove_metric('f1')\n",
    "remove_metric('kappa')\n",
    "remove_metric('mcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, cohen_kappa_score, matthews_corrcoef,\n",
    "    balanced_accuracy_score, log_loss, average_precision_score,\n",
    "    jaccard_score, brier_score_loss, \n",
    ")\n",
    "\n",
    "def register_all_sklearn_metrics():\n",
    "    \n",
    "    \n",
    "    add_metric('auc', 'ROC-AUC',roc_auc_score,target='pred_proba', greater_is_better=True)\n",
    "\n",
    "    add_metric('balanced_acc', 'Balanced Accuracy', balanced_accuracy_score, target='pred',greater_is_better=True)\n",
    "\n",
    "    add_metric('precision', 'Precision',precision_score,target='pred',greater_is_better=True)\n",
    "    \n",
    "    add_metric('recall', 'Recall',recall_score,target='pred',greater_is_better=True)\n",
    "    \n",
    "    add_metric('f1_score', 'F1_score',f1_score,target='pred',greater_is_better=True)\n",
    "    \n",
    "    add_metric('brier', 'Brier Score', brier_score_loss,target='pred_proba', greater_is_better=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    register_all_sklearn_metrics()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "best_model_list = compare_models(sort = 'ROC-AUC',n_select=30,exclude = ['svm',\"ridge\",\"lda\",\"nb\",\"qda\",\"dummy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.SVC(probability=True,random_state=111) \n",
    "SVM = create_model(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "import numpy as np\n",
    "Ridge_1 = RidgeClassifier(random_state=111)\n",
    "\n",
    "\n",
    "alpha_values = np.concatenate([\n",
    "    np.arange(0.1, 1.1, 0.05),  \n",
    "    np.arange(1, 101, 5)      \n",
    "])\n",
    "param_grid_Ridge = {'alpha': alpha_values}\n",
    "Ridge_2 = tune_model(Ridge_1, choose_better = True,optimize=\"ROC-AUC\",custom_grid = param_grid_Ridge,  \n",
    "                     n_iter=100,search_library = 'scikit-learn',search_algorithm = 'grid')  \n",
    "\n",
    "calibrated_model_Ridge = CalibratedClassifierCV(Ridge_2,cv=10,n_jobs=-1)\n",
    "Ridge = create_model(calibrated_model_Ridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "LDA_1 = LinearDiscriminantAnalysis()\n",
    "\n",
    "param_grid_LDA = {'shrinkage': ['auto'], 'solver': ['lsqr', 'eigen']}\n",
    "LDA_2 = tune_model(LDA_1, choose_better = True,optimize=\"ROC-AUC\",custom_grid = param_grid_LDA, \n",
    "                   n_iter=100,search_library = 'scikit-learn',search_algorithm = 'grid') \n",
    "    \n",
    "calibrated_model_LDA = CalibratedClassifierCV(LDA_2,cv=10,n_jobs=-1)\n",
    "LDA = create_model(calibrated_model_LDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB_1 = GaussianNB()\n",
    "\n",
    "param_grid_NB = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "NB_2 = tune_model(NB_1, choose_better = True,optimize=\"ROC-AUC\",custom_grid = param_grid_NB,\n",
    "                  n_iter=100,search_library = 'scikit-learn',search_algorithm = 'grid') \n",
    "\n",
    "calibrated_model_NB = CalibratedClassifierCV(NB_2,cv=10,n_jobs=-1)\n",
    "NB = create_model(calibrated_model_NB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "\n",
    "QDA_1 = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "param_grid_QDA = {'reg_param': np.arange(0.1, 1.05, 0.05)} \n",
    "QDA_2 = tune_model(QDA_1, choose_better = True,optimize=\"ROC-AUC\",custom_grid = param_grid_QDA,\n",
    "                   n_iter=100,search_library = 'scikit-learn',search_algorithm = 'grid') \n",
    "\n",
    "calibrated_model_QDA = CalibratedClassifierCV(QDA_2,cv=10,n_jobs=-1)\n",
    "QDA = create_model(calibrated_model_QDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "GPC = GaussianProcessClassifier(random_state = 234)\n",
    "GPC = create_model(GPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP = MLPClassifier(random_state = 234)\n",
    "MLP = create_model(MLP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_df_1 = get_leaderboard()\n",
    "leaderboard_df_new_1 = leaderboard_df_1.sort_values(by='ROC-AUC',ascending=False)\n",
    "leaderboard_df_new_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select_models_1 = leaderboard_df_new_1['Model']\n",
    "model_select_models_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select_names_1 = leaderboard_df_new_1['Model Name']\n",
    "model_select_names_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "n_1 = len(model_select_models_1)\n",
    "\n",
    "for i in range(n_1):\n",
    "    try:\n",
    "        tuned_model = tune_model(model_select_models_1[i], choose_better = True,optimize=\"ROC-AUC\",\n",
    "                                 n_iter=100,search_library = 'scikit-optimize',search_algorithm = 'bayesian')\n",
    "                            \n",
    "    except:\n",
    "        \n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_df_2 = get_leaderboard()\n",
    "leaderboard_df_new_2 = leaderboard_df_2.sort_values(by='ROC-AUC',ascending=False)\n",
    "leaderboard_df_new_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_df_sorted = leaderboard_df_new_2.sort_values(by='ROC-AUC', ascending=False)\n",
    "leaderboard_df_new_3 = leaderboard_df_sorted.drop_duplicates(subset='Model Name', keep='first')\n",
    "leaderboard_df_new_4 = leaderboard_df_new_3.reset_index(drop=True)\n",
    "leaderboard_df_new_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_column = leaderboard_df_new_4.pop('Model')\n",
    "leaderboard_df_new_4['Model'] = model_column\n",
    "\n",
    "leaderboard_df_new_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_df_new_4.set_index('Model Name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'parm' not in leaderboard_df_new_4.columns:\n",
    "    leaderboard_df_new_4['parm'] = None\n",
    "\n",
    "for index, row in leaderboard_df_new_4.iterrows():\n",
    "    \n",
    "    parm = row['Model'][-1].get_params()\n",
    "    leaderboard_df_new_4.at[index, 'parm'] = str(parm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate Specificity\n",
    "leaderboard_df_new_4['Specificity'] = 2 * leaderboard_df_new_4['Balanced Accuracy'] - leaderboard_df_new_4['Recall']\n",
    "cols = leaderboard_df_new_4.columns.tolist()\n",
    "cols.remove('Specificity')\n",
    "cols.insert(3, 'Specificity')\n",
    "leaderboard_df_new_4 = leaderboard_df_new_4[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_df_new_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models\n",
    "\n",
    "from pycaret.classification import save_model\n",
    "\n",
    "for index, row in leaderboard_df_new_4.iterrows():\n",
    "    save_model(row['Model'],index)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_threshold_input = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path3 = dir_path0 + '3-Model-Result_prob_excel\\\\' \n",
    "if not os.path.exists(dir_path3):\n",
    "    os.makedirs(dir_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_path_test =  dir_path0 + '3-Model-Result_prob_excel\\\\' + f\"3-1-Model-Result_prob_Test-Data-{probability_threshold_input}\\\\\"\n",
    "if not os.path.exists(output_path_test):\n",
    "    os.makedirs(output_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for index, row in leaderboard_df_new_4.iterrows():\n",
    " \n",
    "    pred_model_test_data = predict_model(row['Model'],raw_score=True,probability_threshold=probability_threshold_input)\n",
    "    pred_model_test_data_summary = pull()\n",
    "    model_name = index\n",
    "    \n",
    "    if i == 0:\n",
    "        pred_model_test_data_summary_all = pred_model_test_data_summary\n",
    "    else:\n",
    "        pred_model_test_data_summary_all = pd.concat([pred_model_test_data_summary_all,pred_model_test_data_summary],axis=0)\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_test_data_summary_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_test_data_summary_all = pred_model_test_data_summary_all.sort_values(by='ROC-AUC',ascending=False)\n",
    "pred_model_test_data_summary_all = pred_model_test_data_summary_all.set_index('Model')\n",
    "pred_model_test_data_summary_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_dict = {}\n",
    "\n",
    "for index, row in leaderboard_df_new_4.iterrows():\n",
    "    Model_name = index\n",
    "    model_params_dict[Model_name] = row['parm']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'parm' not in pred_model_test_data_summary_all.columns:\n",
    "    pred_model_test_data_summary_all['parm'] = None\n",
    "\n",
    "for index,row in pred_model_test_data_summary_all.iterrows():\n",
    "    Model_name = index\n",
    "    params = model_params_dict[Model_name]\n",
    "    pred_model_test_data_summary_all.loc[Model_name,'parm'] = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Specificity\n",
    "pred_model_test_data_summary_all['Specificity'] = 2 * pred_model_test_data_summary_all['Balanced Accuracy'] - pred_model_test_data_summary_all['Recall']\n",
    "cols = pred_model_test_data_summary_all.columns.tolist()\n",
    "cols.remove('Specificity')\n",
    "cols.insert(3, 'Specificity')\n",
    "pred_model_test_data_summary_all = pred_model_test_data_summary_all[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_test_data_summary_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve,roc_curve, auc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd_youden = pd.DataFrame(columns=['Model Name','PR-AUC','Max Youden Index ','Cutoff Value', 'ROC-AUC-2'])\n",
    "\n",
    "for filename in os.listdir(output_path_test):\n",
    "    if filename.endswith('.xlsx'):\n",
    "\n",
    "        model_name = filename.split('_prob_Test-Data-0.5.xlsx')[0]\n",
    "\n",
    "        file_path = os.path.join(output_path_test, filename)\n",
    "        df = pd.read_excel(file_path,sheet_name=\"Sheet1\")\n",
    "        \n",
    "        y_true = df['Pathological_Diagnosis']\n",
    "        y_scores = df['prediction_score_1']\n",
    "\n",
    "        precision, recall, thresholds_pr = precision_recall_curve(y_true, y_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        youden_index_max = tpr - fpr\n",
    "        cutoff_value = thresholds[np.argmax(youden_index_max)]\n",
    " \n",
    "        pr_auc = np.round(pr_auc,4)\n",
    "        youden_index_max = np.round(np.max(youden_index_max),4)\n",
    "        cutoff_value = np.round(cutoff_value,4)\n",
    "        roc_auc = np.round(roc_auc,4)\n",
    "        \n",
    "        new_row = pd.DataFrame({\n",
    "            'Model Name': [model_name],\n",
    "            'PR-AUC': [pr_auc],\n",
    "            'Max Youden Index ': [youden_index_max],\n",
    "            'Cutoff Value': [cutoff_value],\n",
    "            'ROC-AUC-2': [roc_auc]\n",
    "        })\n",
    "        pd_youden = pd.concat([pd_youden, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_youden = pd_youden.sort_values(by='ROC-AUC-2', ascending=False)\n",
    "pd_youden.set_index('Model Name', inplace=True)\n",
    "\n",
    "pd_youden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_youden.to_excel(f'{dir_path2}2-3-Test-Data_Youden-Cutoff.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_path_test =  dir_path0 + '3-Model-Result_prob_excel\\\\' + \"3-2-Model-Result_prob_Test-Data_Best-Cutoff\\\\\"\n",
    "if not os.path.exists(output_path_test):\n",
    "    os.makedirs(output_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_youden = pd.read_excel(f'{dir_path2}2-3-Test-Data_Youden-Cutoff.xlsx',sheet_name=\"Sheet1\")\n",
    "pd_youden.set_index('Model Name', inplace=True)\n",
    "pd_youden\n",
    "\n",
    "dict_modelname_cutoff = {}\n",
    "for index, row in pd_youden.iterrows():\n",
    "    model_name = index\n",
    "    cutoff = row['Cutoff Value']\n",
    "    dict_modelname_cutoff[model_name] = cutoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for index, row in leaderboard_df_new_4.iterrows():\n",
    "\n",
    "    cutoff = dict_modelname_cutoff[index]\n",
    "    pred_model_test_data = predict_model(row['Model'],raw_score=True,probability_threshold=float(cutoff))\n",
    "    \n",
    "    pred_model_test_data_summary = pull()\n",
    "    model_name = index\n",
    "    \n",
    "    if i == 0:\n",
    "        pred_model_test_data_summary_all = pred_model_test_data_summary\n",
    "    else:\n",
    "        pred_model_test_data_summary_all = pd.concat([pred_model_test_data_summary_all,pred_model_test_data_summary],axis=0)\n",
    " \n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Specificity\n",
    "pred_model_test_data_summary_all['Specificity'] = 2 * pred_model_test_data_summary_all['Balanced Accuracy'] - pred_model_test_data_summary_all['Recall']\n",
    "cols = pred_model_test_data_summary_all.columns.tolist()\n",
    "cols.remove('Specificity')\n",
    "cols.insert(3, 'Specificity')\n",
    "pred_model_test_data_summary_all = pred_model_test_data_summary_all[cols]\n",
    "\n",
    "\n",
    "pred_model_test_data_summary_all = pred_model_test_data_summary_all.sort_values(by='ROC-AUC',ascending=False)\n",
    "pred_model_test_data_summary_all.set_index('Model', inplace=True)\n",
    "\n",
    "pred_model_test_data_summary_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_test_data_summary_all = pd.merge(pred_model_test_data_summary_all, pd_youden, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'parm' not in pred_model_test_data_summary_all.columns:\n",
    "    pred_model_test_data_summary_all['parm'] = None\n",
    "\n",
    "for index,row in pred_model_test_data_summary_all.iterrows():\n",
    "    Model_name = index\n",
    "    params = model_params_dict[Model_name]\n",
    "    pred_model_test_data_summary_all.loc[Model_name,'parm'] = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_test_data_summary_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_model_test_data_summary_all.to_excel(f'{dir_path2}2-4-Model-Result_Evaluation-Metrics_Test-Data-BestCutoff.xlsx',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#External Validation \n",
    "\n",
    "data_validation = pd.read_excel(r'Model optimization\\2.input\\external validation dataset.xlsx',sheet_name=\"Sheet 1\")\n",
    "\n",
    "data_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_validation['Halo_Sign'] = data_validation['Halo_Sign'].map({'Exists':1,'Absent':0})  \n",
    "data_validation['Gender'] = data_validation['Gender'].map({'Female':1,'Male':0})\n",
    "data_validation['Composition'] = data_validation['Composition'].map({'Solid':1,'Others':0})\n",
    "data_validation['Shape'] = data_validation['Shape'].map({'Microlobulated':1,'Others':0})\n",
    "data_validation['Echogenicity'] = data_validation['Echogenicity'].map({'Hypoechogenicity':1,'Others':0})\n",
    "data_validation['Echogenic_Foci'] = data_validation['Echogenic_Foci'].map({'Microcalcification':1,'Others':0})\n",
    "data_validation['Margin'] = data_validation['Margin'].map({'Irregular':1,'Smooth':0})\n",
    "data_validation['ATR'] = data_validation['ATR'].map({'Taller_than_Wide':1,'Wider_than_Tall':0})\n",
    "data_validation['Pathological_Diagnosis'] = data_validation['Pathological_Diagnosis'].map({'Malignant':1,'Benign':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded1 = pd.get_dummies(data_validation, columns=['Posterior_Echo'], prefix='')\n",
    "df_encoded1.rename(columns={'_Absent_of_Shadowing':'Absent_of_Shadowing','_Posterior_Attenuation':'Posterior_Attenuation','_Shadowing':'Shadowing'},inplace=True)\n",
    "columns_to_convert1 = ['Absent_of_Shadowing','Posterior_Attenuation','Shadowing']\n",
    "df_encoded1[columns_to_convert1] = df_encoded1[columns_to_convert1].astype(int)\n",
    "\n",
    "\n",
    "df_encoded2 = pd.get_dummies(df_encoded1, columns=['Location'], prefix='')\n",
    "df_encoded2.rename(columns={'_Right_Lobe':'Right_Lobe','_Left_Lobe':'Left_Lobe','_Isthmus':'Isthmus'},inplace=True)\n",
    "columns_to_convert2 = ['Right_Lobe','Left_Lobe','Isthmus']\n",
    "df_encoded2[columns_to_convert2] = df_encoded2[columns_to_convert2].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "order_list = ['Intra_BFS','Peri_BFS']\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for i in order_list:\n",
    "    \n",
    "    df_encoded2[i] = label_encoder.fit_transform(df_encoded2[i])\n",
    "\n",
    "    for class_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):\n",
    "        print(f\"{class_label}: {encoded_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = df_encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_features=[\"BMI\",\"Halo_Sign\", \"Absent_of_Shadowing\",\"Posterior_Attenuation\",\"Shadowing\",\n",
    "    \"Right_Lobe\",\"Left_Lobe\",\"Isthmus\",'Intra_BFS']\n",
    "\n",
    "validation_data = validation_data.drop(ignore_features, axis=1)\n",
    "\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for index,row in leaderboard_df_new_4.iterrows():\n",
    "    \n",
    "    cutoff = dict_modelname_cutoff[index]\n",
    "    \n",
    "    prediction = predict_model(row['Model'], data = validation_data ,raw_score=True,probability_threshold=float(cutoff))\n",
    "    \n",
    "    pred_model_external_data_summary = pull()\n",
    "    model_name = index\n",
    "    \n",
    "    if i == 0:\n",
    "        pred_model_external_data_summary_all = pred_model_external_data_summary\n",
    "    else:\n",
    "        pred_model_external_data_summary_all = pd.concat([pred_model_external_data_summary_all,pred_model_external_data_summary],axis=0)\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Specificity\n",
    "pred_model_external_data_summary_all['Specificity'] = 2 * pred_model_external_data_summary_all['Balanced Accuracy'] - pred_model_external_data_summary_all['Recall']\n",
    "cols = pred_model_external_data_summary_all.columns.tolist()\n",
    "cols.remove('Specificity')\n",
    "cols.insert(3, 'Specificity')\n",
    "pred_model_external_data_summary_all = pred_model_external_data_summary_all[cols]\n",
    "\n",
    "pred_model_external_data_summary_all = pred_model_external_data_summary_all.sort_values(by='ROC-AUC',ascending=False)\n",
    "pred_model_external_data_summary_all.set_index('Model', inplace=True)\n",
    "\n",
    "pred_model_external_data_summary_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve,roc_curve, auc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd_youden_1 = pd.DataFrame(columns=['Model Name','PR-AUC','Max Youden Index ','ROC-AUC-2'])\n",
    "\n",
    "for filename in os.listdir(output_path_validation):\n",
    "    if filename.endswith('.xlsx'):  \n",
    "\n",
    "        model_name = filename.split('_prob_Validation-Data-BestCutoff.xlsx')[0]\n",
    "\n",
    "        file_path = os.path.join(output_path_validation, filename)  \n",
    "        df = pd.read_excel(file_path,sheet_name=\"Sheet1\")\n",
    "        \n",
    "        y_true = df['Pathological_Diagnosis']\n",
    "        y_scores = df['prediction_score_1']\n",
    "\n",
    "        precision, recall, thresholds_pr = precision_recall_curve(y_true, y_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        youden_index_max = tpr - fpr\n",
    "         \n",
    "        pr_auc = np.round(pr_auc,4)\n",
    "        youden_index_max = np.round(np.max(youden_index_max),4)\n",
    "        roc_auc = np.round(roc_auc,4)\n",
    "        \n",
    "        new_row = pd.DataFrame({\n",
    "            'Model Name': [model_name],\n",
    "            'PR-AUC': [pr_auc],\n",
    "            'Max Youden Index ': [youden_index_max],\n",
    "            'ROC-AUC-2': [roc_auc]\n",
    "        })\n",
    "        pd_youden_1 = pd.concat([pd_youden_1, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_youde_1 = pd_youden_1.sort_values(by='ROC-AUC-2', ascending=False)\n",
    "pd_youden_1.set_index('Model Name', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_youden_1['Cutoff Value'] = pd_youden['Cutoff Value'].astype(float)\n",
    "pd_youden_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_external_data_summary_all = pd.merge(pred_model_external_data_summary_all, pd_youden_1, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'parm' not in pred_model_external_data_summary_all.columns:\n",
    "    pred_model_external_data_summary_all['parm'] = None\n",
    "\n",
    "for index,row in pred_model_external_data_summary_all.iterrows():\n",
    "    Model_name = index\n",
    "    params = model_params_dict[Model_name]\n",
    "    pred_model_external_data_summary_all.loc[Model_name,'parm'] = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_external_data_summary_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_colors = [\n",
    "    '#1F77B4', \n",
    "    '#FF7F0E',  \n",
    "    '#2CA02C', \n",
    "    '#D62728',\n",
    "    '#9467BD', \n",
    "    '#8C564B', \n",
    "    '#E377C2', \n",
    "    '#7F7F7F', \n",
    "    '#BCBD22', \n",
    "    '#17BECF',\n",
    "    '#4E79A7', \n",
    "    '#F28E2B',  \n",
    "    '#59A14F',  \n",
    "    '#E15759', \n",
    "    '#B07AA1', \n",
    "    '#9C755F', \n",
    "    '#EDC948', \n",
    "    '#76B7B2', \n",
    "    '#FF9DA7', \n",
    "    '#9F9F9F'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文件夹\n",
    "output_path = r\"\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "COLOR_SET = ['#F0FFF0', '#1F77B4']  \n",
    "FONT_COLOR = 'black'                \n",
    "ALPHA = 0.8                        \n",
    "\n",
    "name_list = [\"3-2-Model-Result_prob_Test-Data_Best-Cutoff\",\"3-3-Model-Result_prob_Validation-Data-cutoff\"]\n",
    "\n",
    "for name in name_list:\n",
    "    \n",
    "    path_input = f'Model optimization\\\\3.output\\\\3-Model-Result_prob_excel\\\\{name}\\\\'\n",
    "\n",
    "    for file in os.listdir(path_input):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            model_name = file.split('_')[0]\n",
    "            \n",
    "            data = pd.read_excel(f'{path_input}\\\\{file}')\n",
    "\n",
    "            y_true = data['Pathological_Diagnosis']\n",
    "            y_pred = data['prediction_label']\n",
    "\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "            cmap = LinearSegmentedColormap.from_list('my_cmap', COLOR_SET, N=256)\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(cm_normalized, cmap=cmap, alpha=ALPHA, \n",
    "                    interpolation='nearest', \n",
    "                    vmin=0, vmax=1)  \n",
    "\n",
    "            for i in range(cm.shape[0]):\n",
    "                for j in range(cm.shape[1]):\n",
    "                    plt.text(j, i, f'{cm[i, j]}\\n({cm_normalized[i, j]:.2%})', color=FONT_COLOR,va='center', ha='center',fontsize=12,fontweight='bold')\n",
    "                    \n",
    "            class_names = ['Benign', 'Malignant']\n",
    "            plt.xticks(np.arange(cm.shape[1]), class_names)\n",
    "            plt.yticks(np.arange(cm.shape[0]), class_names)\n",
    "            plt.xlabel('Predicted Label', fontsize=12,labelpad=10)\n",
    "            plt.ylabel('True Label', fontsize=12,labelpad=-6)\n",
    "            if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "                plt.title(f'Confusion Matrix by {model_name} Model in Test Data', pad=10, fontsize=14)\n",
    "            elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "                plt.title(f'Confusion Matrix by {model_name} Model in External Validation Data', pad=10, fontsize=14)\n",
    "\n",
    "\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.set_label('Percentage', rotation=270, labelpad=14,fontsize=12)\n",
    "\n",
    "            \n",
    "            plt.grid(False)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # 保存图片\n",
    "            if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "                plt.savefig(f'{output_path}Test-Data_{model_name}_Confusion-Matrix.svg', format='svg', bbox_inches='tight', dpi=1200, transparent=True)\n",
    "                plt.savefig(f'{output_path}Test-Data_{model_name}_Confusion-Matrix.pdf', format='pdf', bbox_inches='tight', dpi=1200, transparent=True)\n",
    "            elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "                plt.savefig(f'{output_path}Validation-Data_{model_name}_Confusion-Matrix.svg', format='svg', bbox_inches='tight', dpi=1200, transparent=True)\n",
    "                plt.savefig(f'{output_path}Validation-Data_{model_name}_Confusion-Matrix.pdf', format='pdf', bbox_inches='tight', dpi=1200, transparent=True)\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "name_list = [\"3-2-Model-Result_prob_Test-Data_Best-Cutoff\",\"3-3-Model-Result_prob_Validation-Data-cutoff\"]\n",
    "\n",
    "\n",
    "for name in name_list:\n",
    "        \n",
    "        path_input = f'Model optimization\\\\3.output\\\\3-Model-Result_prob_excel\\\\{name}\\\\'\n",
    "\n",
    "        roc_dict = {}\n",
    "        i = 0\n",
    "\n",
    "        for file in os.listdir(path_input):\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                dict1 = {}\n",
    "\n",
    "                model_name = file.split('_')[0]\n",
    "                \n",
    "                data = pd.read_excel(f'{path_input}\\\\{file}')\n",
    "                model_name = file.split('_')[0]\n",
    "\n",
    "                y_true = data['Pathological_Diagnosis']\n",
    "                y_proba = data['prediction_score_1']\n",
    "\n",
    "                dict1['y_proba'] = y_proba\n",
    "                dict1['color'] = main_colors[i]\n",
    "                roc_dict[model_name] = dict1\n",
    "                i = i + 1\n",
    "\n",
    "        roc_dict = dict(sorted(roc_dict.items(), key=lambda item: roc_auc_score(y_true, item[1]['y_proba']), reverse=True))\n",
    "\n",
    "        plt.figure(figsize=(8, 6), dpi=600)\n",
    "\n",
    "        for model_name, model_data in roc_dict.items():\n",
    "            y_proba = model_data['y_proba']\n",
    "            color = model_data['color']\n",
    "            \n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            plt.plot(fpr, tpr, color=color, lw=2, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "            \n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random chance')\n",
    "\n",
    "\n",
    "        plt.xlabel('False Positive Rate (FPR)', fontsize=12) \n",
    "        plt.ylabel('True Positive Rate (TPR)', fontsize=12) \n",
    "        if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "            plt.title(f'ROC Curve by All Model in Test Data', fontsize=12) \n",
    "        elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "            plt.title(f'ROC Curve by All Model in External Validation Data', fontsize=12) \n",
    "        \n",
    "        \n",
    "        plt.legend(labelspacing=0.5,loc='lower right',  fontsize=8, frameon=False)\n",
    "        plt.grid(False)  \n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "        if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "            plt.savefig(f'{output_path}All-Model_ROC-Curve_Test-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "            plt.savefig(f'{output_path}All-Model_ROC-Curve_Test-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "        elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "            plt.savefig(f'{output_path}All-Model_ROC-Curve_Validation-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "            plt.savefig(f'{output_path}All-Model_ROC-Curve_Validation-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "    \n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "name_list = [\"3-2-Model-Result_prob_Test-Data_Best-Cutoff\",\"3-3-Model-Result_prob_Validation-Data-cutoff\"]\n",
    "\n",
    "\n",
    "for name in name_list:\n",
    "        \n",
    "    path_input = f'Model optimization\\\\3.output\\\\3-Model-Result_prob_excel\\\\{name}\\\\'   \n",
    "\n",
    "    pr_dict = {}\n",
    "    i = 0\n",
    "\n",
    "    for file in os.listdir(path_input):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            dict1 = {}\n",
    "\n",
    "            model_name = file.split('_')[0]\n",
    "            \n",
    "            data = pd.read_excel(f'{path_input}\\\\{file}')\n",
    "            model_name = file.split('_')[0]\n",
    "\n",
    "            y_true = data['Pathological_Diagnosis']\n",
    "            y_proba = data['prediction_score_1']\n",
    "\n",
    "            dict1['y_proba'] = y_proba\n",
    "            dict1['color'] = main_colors[i]\n",
    "            pr_dict[model_name] = dict1\n",
    "            i = i + 1\n",
    "\n",
    "    pr_dict = dict(sorted(pr_dict.items(), key=lambda item: roc_auc_score(y_true, item[1]['y_proba']), reverse=True))\n",
    "\n",
    "    plt.figure( dpi=600)\n",
    "\n",
    "    i1 = 0\n",
    "    for model_name, model_data in pr_dict.items():\n",
    "        y_proba = model_data['y_proba']\n",
    "        color = model_data['color']\n",
    "        \n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        plt.plot(recall, precision, color=color, lw=2, label=f'{model_name} (AUC = {pr_auc:.4f})')\n",
    "        \n",
    "\n",
    "    plt.xlim([-0.1, 1.1])\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.xlabel('Recall', fontsize=12 ) \n",
    "    plt.ylabel('Precision', fontsize=12) \n",
    "\n",
    "    if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "        plt.title(f'PR Curve by All Models in Test Data', fontsize=12) \n",
    "    elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "        plt.title(f'PR Curve by All Models in External Validation Data', fontsize=12) \n",
    "\n",
    "    plt.legend(labelspacing=0.4,loc='lower right',  fontsize=8, frameon=False)\n",
    "    plt.grid(False)  \n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "        plt.savefig(f'{output_path}All-Model_PR-Curve_Test-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "        plt.savefig(f'{output_path}All-Model_PR-Curve_Test-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "    elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "        plt.savefig(f'{output_path}All-Model_PR-Curve_Validation-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "        plt.savefig(f'{output_path}All-Model_PR-Curve_Validation-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path =  r\"\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "name_list = [\"3-2-Model-Result_prob_Test-Data_Best-Cutoff\",\"3-3-Model-Result_prob_Validation-Data-cutoff\"]\n",
    "\n",
    "\n",
    "for name in name_list:\n",
    "        \n",
    "    path_input = f'Model optimization\\\\3.output\\\\3-Model-Result_prob_excel\\\\{name}\\\\'\n",
    "\n",
    "    for file in os.listdir(path_input):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            model_name = file.split('_')[0]\n",
    "            \n",
    "            data = pd.read_excel(f'{path_input}\\\\{file}')\n",
    "\n",
    "            y_true = data['Pathological_Diagnosis']\n",
    "            y_proba = data['prediction_score_1']\n",
    "\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_proba, n_bins=10)\n",
    "            brier_score = round(brier_score_loss(y_true, y_proba), 4)\n",
    "\n",
    "            plt.figure(dpi=600)\n",
    "            plt.plot(mean_predicted_value, fraction_of_positives, marker='o', color=\"#AD002AB2\", lw=2, label=f'Model Calibration Curve')\n",
    "            plt.plot([0, 1], [0, 1], color='blue', linestyle='--', label='Perfectly Calibrated Curve')\n",
    "            plt.text(0.752, 0.135, f'Brier Score: {brier_score}', transform=plt.gca().transAxes, fontsize=8, verticalalignment='top', horizontalalignment='left')\n",
    "\n",
    "            plt.xlabel('Mean Predicted Value', fontsize=12) \n",
    "            plt.ylabel('Fraction of Positives', fontsize=12) \n",
    "            \n",
    "            if name == \"3-2-Model-Result_prob_Test-Data_Best-Cutoff\":\n",
    "                plt.title(f'Calibration Curve by {model_name} Model in Test Data', fontsize=12) \n",
    "            elif name == \"3-3-Model-Result_prob_Validation-Data-cutoff\":\n",
    "                plt.title(f'Calibration Curve by {model_name} Model in External Validation Data', fontsize=12 ) \n",
    "\n",
    "            plt.legend(loc='lower right', fontsize=8, labelspacing=0.5, frameon=False)\n",
    "            plt.grid(False)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "                plt.savefig(f'{output_path}{model_name}_Calibration-Curve_Test-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "                plt.savefig(f'{output_path}{model_name}_Calibration-Curve_Test-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "            elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "                plt.savefig(f'{output_path}{model_name}_Calibration-Curve_Validation-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "                plt.savefig(f'{output_path}{model_name}_Calibration-Curve_Validation-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCA curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path =  r''\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "name_list = [\"3-2-Model-Result_prob_Test-Data_Best-Cutoff\",\"3-3-Model-Result_prob_Validation-Data-cutoff\"]\n",
    "\n",
    "\n",
    "for name in name_list:\n",
    "\n",
    "    path_input = f'Model optimization\\\\3.output\\\\3-Model-Result_prob_excel\\\\{name}\\\\'\n",
    "\n",
    "    for file in os.listdir(path_input):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            model_name = file.split('_')[0]\n",
    "            \n",
    "            data = pd.read_excel(f'{path_input}\\\\{file}')\n",
    "\n",
    "            y_labels = data['Pathological_Diagnosis'] \n",
    "            y_pred_scores = data['prediction_score_1']  \n",
    "            \n",
    "            \n",
    "            thresholds = np.arange(0, 1, 0.0001)\n",
    "\n",
    "\n",
    "            y_labels = np.array(y_labels)\n",
    "            y_pred_scores = np.array(y_pred_scores)\n",
    "            n = len(y_labels)\n",
    "            net_benefit_model = np.zeros_like(thresholds)\n",
    "            y_pred_matrix = (y_pred_scores[:, None] > thresholds).astype(int)\n",
    "            tp = (y_pred_matrix & y_labels[:, None]).sum(axis=0)\n",
    "            fp = ((y_pred_matrix == 1) & (y_labels[:, None] == 0)).sum(axis=0)\n",
    "            net_benefit_model = (tp / n) - (fp / n) * (thresholds / (1 - thresholds))\n",
    "            \n",
    "            num_positive = np.sum(y_labels == 1)\n",
    "            num_negative = np.sum(y_labels == 0)\n",
    "            n_total = len(y_labels)\n",
    "            \n",
    "            net_benefit_all = (num_positive / n_total) - (num_negative / n_total) * (thresholds / (1 - thresholds))\n",
    "\n",
    "\n",
    "            # Calculate the intersection point linear interpolation method\n",
    "            def find_intersection_points(thresholds, net_benefit_model, net_benefit_all):\n",
    "                intersection_points1 = []\n",
    "                for i in range(1, len(thresholds)):\n",
    "                    if (net_benefit_model[i-1] - net_benefit_all[i-1]) * (net_benefit_model[i] - net_benefit_all[i]) < 0:\n",
    "                        x1, y1 = thresholds[i-1], net_benefit_model[i-1]\n",
    "                        x2, y2 = thresholds[i], net_benefit_model[i]\n",
    "                        x3, y3 = thresholds[i-1], net_benefit_all[i-1]\n",
    "                        x4, y4 = thresholds[i], net_benefit_all[i]\n",
    "                        det = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n",
    "                        if det != 0:\n",
    "                            x = ((x1*y2 - y1*x2)*(x3 - x4) - (x1 - x2)*(x3*y4 - y3*x4)) / det\n",
    "                            intersection_points1.append((x, (y1 + y2) / 2))\n",
    "                \n",
    "                intersection_points2 = []\n",
    "                for i in range(1, len(thresholds)):\n",
    "                    if (net_benefit_model[i-1] - 0) * (net_benefit_model[i] - 0) < 0:\n",
    "                        x1, y1 = thresholds[i-1], net_benefit_model[i-1]\n",
    "                        x2, y2 = thresholds[i], net_benefit_model[i]\n",
    "                        x = x1 - y1 * (x1 - x2) / (y1 - y2)\n",
    "                        intersection_points2.append((x, 0))\n",
    "                \n",
    "                return intersection_points1, intersection_points2\n",
    "\n",
    "            intersection_points1, intersection_points2= find_intersection_points(thresholds, net_benefit_model, net_benefit_all)\n",
    "            \n",
    "            fig, ax = plt.subplots(dpi=600)\n",
    "            ax.plot(thresholds, net_benefit_model, color='deepskyblue', label= \"Model\")\n",
    "            ax.plot(thresholds, net_benefit_all, color='black', label='Treat all')\n",
    "            ax.plot((0, 1), (0, 0), color='#808080', label='Treat none')\n",
    "\n",
    "            y2 = np.maximum(net_benefit_all, 0)\n",
    "            y1 = np.maximum(net_benefit_model, y2)\n",
    "            ax.fill_between(thresholds, y1, y2, color='deepskyblue', alpha=0.3)\n",
    "\n",
    "            i = 0\n",
    "            for point in intersection_points1:\n",
    "                x, y = point\n",
    "                ax.scatter(x, y, color='red', s=20, zorder=2.0)\n",
    "                ax.text(x, y + i, f'Threshold Probability: {x:.4f}', fontsize=8, color='red') \n",
    "                i = i - 0.05\n",
    "            i = 0\n",
    "            for point in intersection_points2:\n",
    "                x, y = point\n",
    "                ax.scatter(x, y, color='red', s=20, zorder=2.0)\n",
    "                ax.text(x, y + i, f'Threshold Probability: {x:.4f}', fontsize=8, color='red')\n",
    "                i = i - 0.05\n",
    "\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(-0.15, 1.15)\n",
    "            ax.set_xlabel('Threshold Probability', fontsize=12 ) \n",
    "            ax.set_ylabel('Net Benefit', fontsize=12) \n",
    "            ax.grid(False)\n",
    "            ax.legend(loc='upper right', fontsize=8, frameon=False)\n",
    "            \n",
    "            if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "                plt.title(f'DCA Curve by {model_name} in Test Data', fontsize=12) \n",
    "            elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "                plt.title(f'DCA Curve by {model_name} in External Validation Data', fontsize=12) \n",
    "            \n",
    "            plt.tight_layout()\n",
    "\n",
    "            if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "                plt.savefig(f'{output_path}{model_name}_DCA-Curve_Test-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "                plt.savefig(f'{output_path}{model_name}_DCA-Curve_Test-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "            elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "                plt.savefig(f'{output_path}{model_name}_DCA-Curve_Validation-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "                plt.savefig(f'{output_path}{model_name}_DCA-Curve_Validation-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "        \n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_path = r\"\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "name_list = [\"3-2-Model-Result_prob_Test-Data_Best-Cutoff\",\"3-3-Model-Result_prob_Validation-Data-cutoff\"]\n",
    "\n",
    "\n",
    "for name in name_list:\n",
    "        \n",
    "    path_input = f'Model optimization\\\\3.output\\\\3-Model-Result_prob_excel\\\\{name}\\\\'\n",
    "\n",
    "    for file in os.listdir(path_input):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            model_name = file.split('_')[0]\n",
    "            \n",
    "            data = pd.read_excel(f'{path_input}\\\\{file}')\n",
    "\n",
    "            y_labels = data['Pathological_Diagnosis'] \n",
    "            y_pred_scores = data['prediction_score_1'] \n",
    "\n",
    "            fpr, tpr, thresholds= roc_curve(y_labels, y_pred_scores)\n",
    "            ks_value = max(abs(tpr-fpr)) \n",
    "            \n",
    "            plt.figure(dpi = 600)\n",
    "            plt.plot(thresholds, abs(tpr-fpr), label='TPR-FPR', color=main_colors[3], linewidth=2.5, alpha=0.8)\n",
    "\n",
    "            plt.plot(thresholds, tpr, label='True Positive Rate (TPR)', color=main_colors[0], linewidth=2.5, alpha=0.8)\n",
    "            plt.plot(thresholds, fpr, label='False Positive Rate (FPR)', color=main_colors[7], linewidth=2.5, alpha=0.8)\n",
    "    \n",
    "            plt.xlabel('Thresholds', fontsize=12)\n",
    "\n",
    "            plt.ylabel('Rate', fontsize=12)\n",
    "\n",
    "            plt.xlim(0.0, 1.0)\n",
    "            plt.ylim(0.0, 1.0)\n",
    "\n",
    "            if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "                plt.title(f'KS Curve by {model_name} in Test Data', fontsize=12) \n",
    "            elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "                plt.title(f'KS Curve by {model_name} in External Validation Data', fontsize=12) \n",
    "               \n",
    "            plt.grid(False)\n",
    "            plt.legend(labelspacing=0.5,loc='lower left', fontsize=10, frameon=False)\n",
    "\n",
    "            idx = np.argwhere(abs(fpr-tpr) == ks_value)[0, 0]\n",
    "            ks_thresholds = thresholds[idx]\n",
    "\n",
    "            plt.plot((ks_thresholds, ks_thresholds), (fpr[idx], tpr[idx]), \n",
    "                    label='KS - {:.4f}'.format(ks_value), \n",
    "                    color=main_colors[3], linestyle='--', linewidth=2.5, marker='o', \n",
    "                    markerfacecolor=main_colors[3], markersize=8)\n",
    "\n",
    "            plt.annotate(f'KS Value: {ks_value:.4f}\\nThreshold: {ks_thresholds:.4f}', \n",
    "                        xy=(ks_thresholds, (fpr[idx] + tpr[idx]) / 2), \n",
    "                        xytext=(ks_thresholds + 0.05, (fpr[idx] + tpr[idx]) / 2 + 0.05),\n",
    "                        arrowprops=dict(facecolor=main_colors[3], shrink=0.05),\n",
    "                        fontsize=12) \n",
    "\n",
    "            plt.scatter((ks_thresholds, ks_thresholds), (fpr[idx], tpr[idx]), color=main_colors[3], s=100, zorder=5)    \n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if name == '3-2-Model-Result_prob_Test-Data_Best-Cutoff':\n",
    "                plt.savefig(f'{output_path}{model_name}_KS-Curve_Test-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "                plt.savefig(f'{output_path}{model_name}_KS-Curve_Test-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "            elif name == '3-3-Model-Result_prob_Validation-Data-cutoff':\n",
    "                plt.savefig(f'{output_path}{model_name}_KS-Curve_Validation-Data.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "                plt.savefig(f'{output_path}{model_name}_KS-Curve_Validation-Data.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "        \n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_path = r''\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "X_train_transformed = pd.read_excel(r\"X_train_transformed.xlsx\", index_col=0)\n",
    "y_train_transformed = pd.read_excel(r\"y_train_transformed.xlsx\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_input  =  r\"\" # Model_pkl path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "import os\n",
    "for file in os.listdir(model_path_input):\n",
    "    if file.endswith('.pkl'):\n",
    "        model_name  =  file.split('.')[0]\n",
    "        model_path = model_path_input + model_name\n",
    "        \n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, X_train_transformed, y_train_transformed,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 50),\n",
    "            cv=10,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=5,\n",
    "            random_state=42,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        \n",
    "        n_cv_folds = train_scores.shape[1] \n",
    "        train_scores_se = train_scores_std / np.sqrt(n_cv_folds)\n",
    "        test_scores_se = test_scores_std / np.sqrt(n_cv_folds)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6), dpi=600)           \n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color= main_colors[0], label=\"Training Score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color= main_colors[1], label=\"Cross Validation Score\")\n",
    "\n",
    "        plt.fill_between(train_sizes, train_scores_mean - 1.96*train_scores_se,\n",
    "                         train_scores_mean + 1.96*train_scores_se, alpha=0.2, color= main_colors[0], label=\"Training Score 95% CI\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - 1.96*test_scores_se,\n",
    "                         test_scores_mean + 1.96*test_scores_se, alpha=0.2, color= main_colors[1], label=\"Cross Validation Score 95% CI\")\n",
    "\n",
    "\n",
    "        plt.gca().yaxis.set_major_formatter(plt.FormatStrFormatter('%.4f'))\n",
    "        plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.02))\n",
    "        plt.gca().yaxis.set_major_locator(plt.MaxNLocator(nbins=10))\n",
    "\n",
    "        plt.xlabel('Data Size', fontsize=12) \n",
    "        plt.ylabel('Area Under the ROC Curve', fontsize=12) \n",
    "        plt.title(f\"Learning Curve by {model_name} with 95% Confidence Interval\", fontsize=12)\n",
    "        plt.legend(loc=\"best\", fontsize=12)\n",
    "        plt.grid(False)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(f'{output_path}Learning_Curve_by_{model_name}.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "        plt.savefig(f'{output_path}Learning_Curve_by_{model_name}.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "       \n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP\n",
    "# The best model is logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_path = r\"\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_test_transformed = pd.read_excel(r\"X_test_transformed.xlsx\",index_col=0) \n",
    "y_test_transformed = pd.read_excel(r\"y_test_transformed.xlsx\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import load_model\n",
    "\n",
    "model_input = load_model(r\"Web Application\\Logistic Regression\")\n",
    "best_model = model_input[-1]\n",
    "\n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.SamplingExplainer(best_model.predict, X_test_transformed) \n",
    "shap_values = explainer.shap_values(X_test_transformed,n_jobs=-2)\n",
    "\n",
    "shap_values2 = explainer(X_test_transformed) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = plt.gcf()\n",
    "shap.plots.bar(shap_values2[14],show_data=True,max_display=20) \n",
    "\n",
    "\n",
    "fig5.savefig(f'{output_path}8-Local-Summary-Bar_Test-Data-14.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "fig5.savefig(f'{output_path}8-Local-Summary-Bar_Test-Data-14.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig55 = plt.gcf()\n",
    "shap.plots.bar(shap_values2[167],show_data=True,max_display=20) \n",
    "\n",
    "\n",
    "fig55.savefig(f'{output_path}8-Local-Summary-Bar_Test-Data-167.svg', format='svg', bbox_inches='tight', dpi=1200)\n",
    "fig55.savefig(f'{output_path}8-Local-Summary-Bar_Test-Data-167.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thyroid_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
